# -*- coding: utf-8 -*-
import os, re, io, time
from urllib.parse import urlparse, parse_qs
from collections import Counter

import requests
import pandas as pd
import streamlit as st
import plotly.express as px
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

# ====================== –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ======================
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"))
st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", page_icon="üí¨", layout="wide")
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å—Ç–∏–ª–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫ (—á—Ç–æ–±—ã –≤—ã–≥–ª—è–¥–µ–ª–æ –∫—Ä–∞—Å–∏–≤–æ –∏ –Ω–∞ –±–µ–ª–æ–π —Ç–µ–º–µ)
# üíÖ –°—Ç–∏–ª–∏ –º–µ—Ç—Ä–∏–∫ (—á—Ç–æ–±—ã –≤—ã–≥–ª—è–¥–µ–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏ –Ω–∞ –±–µ–ª–æ–π —Ç–µ–º–µ)
st.markdown("""
    <style>
    /* –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –º–µ—Ç—Ä–∏–∫ */
    div[data-testid="stMetricValue"] {
        color: #111 !important;
    }
    div[data-testid="stMetricLabel"] {
        color: #333 !important;
        font-weight: 600 !important;
    }
    /* –í–µ—Å—å –±–ª–æ–∫ –º–µ—Ç—Ä–∏–∫–∏ (—Ñ–æ–Ω –∏ —Ç–µ–Ω—å) */
    div[data-testid="stMetric"] {
        background-color: #ffffff !important;
        border: 1px solid #ddd !important;
        border-radius: 12px !important;
        padding: 15px !important;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    /* –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–π –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π overlay –æ—Ç —Ç–µ–º—ã */
    [data-testid="stMetricDelta"] {
        color: #444 !important;
    }
    </style>
""", unsafe_allow_html=True)


VK_API_VERSION = "5.131"

# ====================== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –∏ API ======================
with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API –∏ –º–æ–¥–µ–ª–∏"):
    vk_token_ui = st.text_input("VK_TOKEN", os.getenv("VK_TOKEN", ""), type="password")
    yt_ui = st.text_input("YOUTUBE_API_KEY", os.getenv("YOUTUBE_API_KEY", ""), type="password")
    model_choice = st.selectbox(
        "–ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:",
        [
            "cointegrated/rubert-tiny-sentiment-balanced (–±—ã—Å—Ç—Ä–∞—è)",
            "cointegrated/rubert-base-cased-sentiment-balanced (—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)",
            "blanchefort/rubert-base-cased-sentiment (—Ç–æ—á–Ω–∞—è)"
        ],
        index=0
    )

# ====================== –ú–æ–¥–µ–ª—å ======================
@st.cache_resource(show_spinner=False)
def load_pipeline(model_name: str):
    tok = AutoTokenizer.from_pretrained(model_name)
    mdl = AutoModelForSequenceClassification.from_pretrained(model_name)
    return TextClassificationPipeline(model=mdl, tokenizer=tok, top_k=None, truncation=True, device=-1)

def classify_many(texts: list[str], model_name: str) -> list[str]:
    clf = load_pipeline(model_name)
    out_labels = []
    for t in texts:
        t = (t or "").strip()
        if not t:
            out_labels.append("–ù–µ–π—Ç—Ä–∞–ª")
            continue
        out = clf(t[:512])[0]
        best = max(out, key=lambda x: x["score"])
        lab = best["label"].upper()
        if lab == "POSITIVE":
            out_labels.append("–ü–æ–∑–∏—Ç–∏–≤")
        elif lab == "NEGATIVE":
            out_labels.append("–ù–µ–≥–∞—Ç–∏–≤")
        else:
            out_labels.append("–ù–µ–π—Ç—Ä–∞–ª")
    return out_labels

# ====================== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ ======================
def detect_platform(url: str) -> str:
    host = (urlparse(url).hostname or "").lower()
    if "vk.com" in host:
        return "VK"
    if "youtube.com" in host or "youtu.be" in host:
        return "YOUTUBE"
    return "UNKNOWN"

# VK
VK_WALL_RE = re.compile(r"wall(?P<owner>-?\d+)_(?P<post>\d+)")
def vk_extract_ids(url: str):
    m = VK_WALL_RE.search(url)
    if not m:
        q = parse_qs(urlparse(url).query).get("w", [""])[0]
        m = VK_WALL_RE.search(q)
    if not m:
        raise ValueError("VK: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å owner_id –∏ post_id.")
    return int(m.group("owner")), int(m.group("post"))

def vk_call(token: str, method: str, **params):
    params.update({"access_token": token, "v": VK_API_VERSION})
    r = requests.get(f"https://api.vk.com/method/{method}", params=params, timeout=30)
    j = r.json()
    if "error" in j:
        e = j["error"]
        raise RuntimeError(f"VK API error {e.get('error_code')}: {e.get('error_msg')}")
    return j["response"]

def fetch_vk_comments(token: str, url: str):
    owner_id, post_id = vk_extract_ids(url)
    all_comments, offset = [], 0
    while True:
        resp = vk_call(
            token, "wall.getComments",
            owner_id=owner_id, post_id=post_id,
            count=100, offset=offset,
            sort="asc", thread_items_count=10, extended=0
        )
        items = resp.get("items", [])
        if not items:
            break
        all_comments.extend(items)
        offset += len(items)
        if len(items) < 100:
            break
        time.sleep(0.25)

    texts, ids, parents, dates = [], [], [], []
    for it in all_comments:
        t = (it.get("text") or "").strip()
        if t:
            texts.append(t)
            ids.append(it.get("id"))
            parents.append(None)
            dates.append(pd.to_datetime(it.get("date", 0), unit="s", utc=True))
        for r in (it.get("thread") or {}).get("items", []):
            tr = (r.get("text") or "").strip()
            if tr:
                texts.append(tr)
                ids.append(r.get("id"))
                parents.append(it.get("id"))
                dates.append(pd.to_datetime(r.get("date", 0), unit="s", utc=True))
    return ids, parents, texts, dates

# YouTube
def extract_youtube_id(url: str) -> str:
    q = parse_qs(urlparse(url).query)
    if "v" in q:
        return q["v"][0]
    m = re.search(r"youtu\.be/([A-Za-z0-9_-]+)", url)
    if m:
        return m.group(1)
    raise ValueError("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å YouTube video ID.")

def fetch_youtube_comments(api_key: str, url: str):
    video_id = extract_youtube_id(url)
    comments, ids, parents, dates = [], [], [], []
    next_page = None
    while True:
        r = requests.get(
            "https://www.googleapis.com/youtube/v3/commentThreads",
            params={
                "part": "snippet",
                "videoId": video_id,
                "maxResults": 100,
                "key": api_key,
                "pageToken": next_page,
            },
            timeout=30,
        ).json()
        for item in r.get("items", []):
            sn = item["snippet"]["topLevelComment"]["snippet"]
            txt = (sn.get("textDisplay") or "").strip()
            if txt:
                comments.append(txt)
                ids.append(item["id"])
                parents.append(None)
                dates.append(pd.to_datetime(sn["publishedAt"], utc=True))
        next_page = r.get("nextPageToken")
        if not next_page:
            break
        time.sleep(0.2)
    return ids, parents, comments, dates

# ====================== –ê–Ω–∞–ª–∏–∑ ======================
def analyze_one(url: str, vk_token: str, yt_key: str, model_name: str):
    platform = detect_platform(url)
    if platform == "VK":
        ids, parents, texts, dates = fetch_vk_comments(vk_token, url)
    elif platform == "YOUTUBE":
        ids, parents, texts, dates = fetch_youtube_comments(yt_key, url)
    else:
        raise RuntimeError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞.")
    labels = classify_many(texts, model_name)
    df = pd.DataFrame({
        "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞": platform,
        "–°—Å—ã–ª–∫–∞": url,
        "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ID": ids,
        "–†–æ–¥–∏—Ç–µ–ª—å ID": parents,
        "–¢–µ–∫—Å—Ç": texts,
        "–î–∞—Ç–∞": dates,
        "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": labels
    })
    cnt = Counter(labels)
    total = max(1, len(labels))
    summary = {
        "–ü–æ–∑–∏—Ç–∏–≤ (%)": round(100 * cnt.get("–ü–æ–∑–∏—Ç–∏–≤", 0) / total, 1),
        "–ù–µ–π—Ç—Ä–∞–ª (%)": round(100 * cnt.get("–ù–µ–π—Ç—Ä–∞–ª", 0) / total, 1),
        "–ù–µ–≥–∞—Ç–∏–≤ (%)": round(100 * cnt.get("–ù–µ–≥–∞—Ç–∏–≤", 0) / total, 1),
        "–ü–æ–∑–∏—Ç–∏–≤": cnt.get("–ü–æ–∑–∏—Ç–∏–≤", 0),
        "–ù–µ–π—Ç—Ä–∞–ª": cnt.get("–ù–µ–π—Ç—Ä–∞–ª", 0),
        "–ù–µ–≥–∞—Ç–∏–≤": cnt.get("–ù–µ–≥–∞—Ç–∏–≤", 0),
        "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤": len(texts),
    }
    return summary, df

# ====================== UI ======================
urls_raw = st.text_area(
    "–°—Å—ã–ª–∫–∏ (VK, YouTube ‚Äî –ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)",
    height=150,
    placeholder="https://vk.com/wall-141155426_420521\nhttps://www.youtube.com/watch?v=dQw4w9WgXcQ",
)
show_table = st.checkbox("–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤", value=False)
go = st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å", type="primary")

if "summary_df" not in st.session_state:
    st.session_state["summary_df"] = None
if "all_df" not in st.session_state:
    st.session_state["all_df"] = None

if go:
    urls = [u.strip() for u in (urls_raw or "").splitlines() if u.strip()]
    progress = st.progress(0.0)
    per_link, frames = [], []
    for i, url in enumerate(urls, start=1):
        try:
            s, df = analyze_one(url, vk_token_ui, yt_ui, model_choice.split()[0])
            per_link.append({"–°—Å—ã–ª–∫–∞": url, "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞": df.iloc[0]["–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞"], **s})
            frames.append(df)
        except Exception as e:
            per_link.append({"–°—Å—ã–ª–∫–∞": url, "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞": "?", "–û—à–∏–±–∫–∞": str(e)})
        progress.progress(i / len(urls))
    if frames:
        st.session_state["summary_df"] = pd.DataFrame(per_link)
        st.session_state["all_df"] = pd.concat(frames, ignore_index=True)

# ====================== –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ======================
if st.session_state["summary_df"] is not None:
    summary_df = st.session_state["summary_df"]
    all_df = st.session_state["all_df"]

    # –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—â–µ–π —Å–≤–æ–¥–∫–∏
    st.subheader("üìä –û–±—â–∞—è —Å–≤–æ–¥–∫–∞")
    counts = all_df["–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"].value_counts().to_dict()
    total = max(1, len(all_df))
    totals = {
        "–ü–æ–∑–∏—Ç–∏–≤": (round(100 * counts.get("–ü–æ–∑–∏—Ç–∏–≤", 0) / total, 1), counts.get("–ü–æ–∑–∏—Ç–∏–≤", 0), "green"),
        "–ù–µ–π—Ç—Ä–∞–ª": (round(100 * counts.get("–ù–µ–π—Ç—Ä–∞–ª", 0) / total, 1), counts.get("–ù–µ–π—Ç—Ä–∞–ª", 0), "gold"),
        "–ù–µ–≥–∞—Ç–∏–≤": (round(100 * counts.get("–ù–µ–≥–∞—Ç–∏–≤", 0) / total, 1), counts.get("–ù–µ–≥–∞—Ç–∏–≤", 0), "red"),
    }

    # —Ç—Ä–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    c1, c2, c3 = st.columns(3)
    for col, (label, (p, n, color)) in zip([c1, c2, c3], totals.items()):
        col.markdown(
            f"""
            <style>
            @media (prefers-color-scheme: light) {{
                .metric-card {{
                    background-color: #ffffff;
                    border: 1px solid #e6e6e6;
                    color: #333333;
                }}
            }}
            @media (prefers-color-scheme: dark) {{
                .metric-card {{
                    background-color: #2b2b2b;
                    border: 1px solid #444444;
                    color: #dddddd;
                }}
            }}
            </style>

            <div class="metric-card" style="
                border-radius:12px;
                padding:1.5rem;
                text-align:center;
                box-shadow:0 2px 6px rgba(0,0,0,0.15);
                transition:transform 0.2s ease;
            ">
                <h4 style="color:{color}; margin-bottom:0.4rem;">{label}</h4>
                <h2 style="color:{color}; margin:0; font-size:2.2rem;">{p}%</h2>
                <p style="margin-top:0.3rem; font-size:14px;">{n} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤</p>
            </div>
            """,
            unsafe_allow_html=True
        )




    # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    pie_df = pd.DataFrame({"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": list(totals.keys()), "–ü—Ä–æ—Ü–µ–Ω—Ç": [v[0] for v in totals.values()]})
    fig = px.pie(
        pie_df, names="–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", values="–ü—Ä–æ—Ü–µ–Ω—Ç", hole=0.35,
        color="–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", color_discrete_map={"–ü–æ–∑–∏—Ç–∏–≤":"green","–ù–µ–π—Ç—Ä–∞–ª":"gold","–ù–µ–≥–∞—Ç–∏–≤":"red"},
        title="–û–±—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º"
    )
    st.plotly_chart(fig, use_container_width=True)


    # ---------- –ì—Ä–∞—Ñ–∏–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º ----------
    st.subheader("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Å—ã–ª–∫–∞–º")
    fig2 = px.bar(
        summary_df.melt(id_vars=["–°—Å—ã–ª–∫–∞", "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞"], value_vars=["–ü–æ–∑–∏—Ç–∏–≤", "–ù–µ–π—Ç—Ä–∞–ª", "–ù–µ–≥–∞—Ç–∏–≤"]),
        x="–°—Å—ã–ª–∫–∞", y="value", color="variable", barmode="stack",
        color_discrete_map={"–ü–æ–∑–∏—Ç–∏–≤":"green","–ù–µ–π—Ç—Ä–∞–ª":"gold","–ù–µ–≥–∞—Ç–∏–≤":"red"},
        labels={"value":"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ","variable":"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"}
    )
    st.plotly_chart(fig2, use_container_width=True)

    # ---------- –ì—Ä–∞—Ñ–∏–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ----------
    st.subheader("üïí –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
    if "–î–∞—Ç–∞" in all_df.columns:
        tmp = all_df.dropna(subset=["–î–∞—Ç–∞"]).copy()
        if tmp["–î–∞—Ç–∞"].dt.tz is None:
            tmp["–î–∞—Ç–∞"] = tmp["–î–∞—Ç–∞"].dt.tz_localize("UTC")
        tmp["–î–∞—Ç–∞"] = tmp["–î–∞—Ç–∞"].dt.tz_convert(None)
        tmp["–î–∞—Ç–∞_–¥–µ–Ω—å"] = tmp["–î–∞—Ç–∞"].dt.date
        time_df = tmp.groupby(["–î–∞—Ç–∞_–¥–µ–Ω—å", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"]).size().reset_index(name="count")
        fig_time = px.line(
            time_df, x="–î–∞—Ç–∞_–¥–µ–Ω—å", y="count", color="–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å",
            color_discrete_map={"–ü–æ–∑–∏—Ç–∏–≤":"green","–ù–µ–π—Ç—Ä–∞–ª":"gold","–ù–µ–≥–∞—Ç–∏–≤":"red"},
            markers=True
        )
        st.plotly_chart(fig_time, use_container_width=True)

    # ---------- –°–≤–æ–¥–∫–∞ –∏ —Ç–∞–±–ª–∏—Ü–∞ ----------
    st.subheader("üìä –°–≤–æ–¥–∫–∞ –ø–æ —Å—Å—ã–ª–∫–∞–º")
    st.dataframe(summary_df, use_container_width=True, height=300)
    if show_table:
        st.subheader("üí¨ –í—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏")
        st.dataframe(all_df, use_container_width=True, height=430)

    # ---------- –í—ã–≥—Ä—É–∑–∫–∞ ----------
    st.markdown("### üíæ –í—ã–≥—Ä—É–∑–∫–∞")
    csv_data = all_df.to_csv(index=False).encode("utf-8")
    st.download_button("üìÑ –°–∫–∞—á–∞—Ç—å CSV", data=csv_data, file_name="sentiment_comments.csv", mime="text/csv")

    # Excel (2 –ª–∏—Å—Ç–∞)
    xls_buf = io.BytesIO()
    try:
        if all_df["–î–∞—Ç–∞"].dt.tz is not None:
            all_df["–î–∞—Ç–∞"] = all_df["–î–∞—Ç–∞"].dt.tz_convert(None)
    except Exception:
        pass
    with pd.ExcelWriter(xls_buf, engine="openpyxl") as writer:
        summary_df.to_excel(writer, sheet_name="–°–≤–æ–¥–∫–∞", index=False)
        all_df.to_excel(writer, sheet_name="–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏", index=False)
    xls_buf.seek(0)
    st.download_button(
        "üìä –°–∫–∞—á–∞—Ç—å Excel",
        data=xls_buf,
        file_name="sentiment_results.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
else:
    st.info("–î–æ–±–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ –∏ –Ω–∞–∂–º–∏—Ç–µ ¬´–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª.")
